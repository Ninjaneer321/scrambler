{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, CuDNNLSTM, CuDNNGRU, BatchNormalization, LocallyConnected2D, Permute, TimeDistributed, Bidirectional\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "from sequence_logo_helper_protein import plot_protein_logo\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n",
    "\n",
    "class EpochVariableCallback(Callback) :\n",
    "    \n",
    "    def __init__(self, my_variable, my_func) :\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}) :\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n",
    "\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            val : key for key, val in channel_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            if max_nt == 1 :\n",
    "                seq += self.decode_map[argmax_nt]\n",
    "            else :\n",
    "                seq += self.decode_map[-1]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        encoding = np.array(encoding_mat[row_index, :].todense()).reshape(-1, 4)\n",
    "        return self.decode(encoding)\n",
    "\n",
    "class NopTransformer(iso.ValueTransformer) :\n",
    "    \n",
    "    def __init__(self, n_classes) :\n",
    "        super(NopTransformer, self).__init__('nop', (n_classes, ))\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def transform(self, values) :\n",
    "        return values\n",
    "    \n",
    "    def transform_inplace(self, values, transform) :\n",
    "        transform[:] = values\n",
    "    \n",
    "    def transform_inplace_sparse(self, values, transform_mat, row_index) :\n",
    "        transform_mat[row_index, :] = np.ravel(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pair_df) = 890124\n",
      "   Unnamed: 0                                       monomer_id_1  \\\n",
      "0           0  redesigned_closed_6_6_9_9middlesbobby_1_4_S_01...   \n",
      "1           1  redesigned_closed_6_8_9_9middlesbobby_1_4_S_40...   \n",
      "2           2  redesigned_closed_6_6_9_10middlesbobby_1_4_S_1...   \n",
      "3           3  redesigned_closed_5_7_9_9middlesbobby_1_1_S_43...   \n",
      "4           4  redesigned_closed_6_6_8_10middlesbobby_1_5_S_2...   \n",
      "\n",
      "                                        monomer_id_2  \\\n",
      "0  redesigned_closed_6_6_8_10middlesbobby_1_5_S_2...   \n",
      "1  redesigned_closed_5_7_8_10middlesbobby_1_2_S_0...   \n",
      "2  redesigned_closed_6_6_9_9middlesbobby_1_4_S_07...   \n",
      "3  redesigned_closed_5_6_9_9middlesbobby_1_1_S_25...   \n",
      "4  redesigned_closed_6_6_8_9middlesbobby_1_5_S_16...   \n",
      "\n",
      "                                         amino_seq_1  \\\n",
      "0  SEKDLLRLNREILEEIERIQKDLEELLERAERDAEGGLEELEKLVR...   \n",
      "1  SEKEVMKEQIRLIRENIKAQEEILRLLKELERKGVDKEVEEVIKRI...   \n",
      "2  DEEEILKILEENLRIQREIDRIHEEQVKALERITRRREDREEIEKL...   \n",
      "3  STEDIARELRKIIRRDKESKKEIKRVHDEQRELAKDAEDSRVVRRL...   \n",
      "4  SEKEIIKRLNKLNEDLTRLLETYRRLVEEVERAGALEEELRRRQRE...   \n",
      "\n",
      "                                         amino_seq_2  interacts  \n",
      "0  TSEENVERQREHVRTTDEAIKEMEKIIRLLEVVARGEMDRDELRKV...        0.0  \n",
      "1  STEEVERIVEEVERISRRVVEISRRVVEKIRELIRRMKNERLVELL...        0.0  \n",
      "2  RTRELLDEHRKLLEEQERQTKQDEELLREVERRLREELIEMAKDVQ...        0.0  \n",
      "3  GKEEVLEVAKRLLELQEKLQRLHEELQRILDDIVRRKNADDTLVRR...        0.0  \n",
      "4  SEKEELKRLLEESNKLLELVKEQLRLAEDALRKIAKKARGEVEILE...        0.0  \n",
      "Training set size = 801112\n",
      "Validation set size = 445\n",
      "Test set size = 88567\n"
     ]
    }
   ],
   "source": [
    "#Re-load cached dataframe (shuffled)\n",
    "\n",
    "dataset_name = \"coiled_coil_binders\"\n",
    "\n",
    "experiment = \"baker_big_set_5x_negatives\"\n",
    "\n",
    "pair_df = pd.read_csv(\"pair_df_\" + experiment + \"_in_shuffled.csv\", sep=\"\\t\")\n",
    "\n",
    "print(\"len(pair_df) = \" + str(len(pair_df)))\n",
    "\n",
    "print(pair_df.head())\n",
    "\n",
    "#Generate training and test set indexes\n",
    "valid_set_size = 0.0005\n",
    "test_set_size = 0.0995\n",
    "\n",
    "data_index = np.arange(len(pair_df), dtype=np.int)\n",
    "\n",
    "train_index = data_index[:-int(len(pair_df) * (valid_set_size + test_set_size))]\n",
    "valid_index = data_index[train_index.shape[0]:-int(len(pair_df) * test_set_size)]\n",
    "test_index = data_index[train_index.shape[0] + valid_index.shape[0]:]\n",
    "\n",
    "print('Training set size = ' + str(train_index.shape[0]))\n",
    "print('Validation set size = ' + str(valid_index.shape[0]))\n",
    "print('Test set size = ' + str(test_index.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 40000\n",
      "Test set size = 4000\n"
     ]
    }
   ],
   "source": [
    "#Sub-select smaller dataset\n",
    "\n",
    "n_train_pos = 20000\n",
    "n_train_neg = 20000\n",
    "\n",
    "n_test_pos = 2000\n",
    "n_test_neg = 2000\n",
    "\n",
    "orig_n_train = train_index.shape[0]\n",
    "orig_n_valid = valid_index.shape[0]\n",
    "orig_n_test = test_index.shape[0]\n",
    "\n",
    "train_index_pos = np.nonzero((pair_df.iloc[train_index]['interacts'] == 1).values)[0][:n_train_pos]\n",
    "train_index_neg = np.nonzero((pair_df.iloc[train_index]['interacts'] == 0).values)[0][:n_train_neg]\n",
    "\n",
    "train_index = np.concatenate([train_index_pos, train_index_neg], axis=0)\n",
    "np.random.shuffle(train_index)\n",
    "\n",
    "test_index_pos = np.nonzero((pair_df.iloc[test_index]['interacts'] == 1).values)[0][:n_test_pos] + orig_n_train + orig_n_valid\n",
    "test_index_neg = np.nonzero((pair_df.iloc[test_index]['interacts'] == 0).values)[0][:n_test_neg] + orig_n_train + orig_n_valid\n",
    "\n",
    "test_index = np.concatenate([test_index_pos, test_index_neg], axis=0)\n",
    "np.random.shuffle(test_index)\n",
    "\n",
    "print('Training set size = ' + str(train_index.shape[0]))\n",
    "print('Test set size = ' + str(test_index.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate sequence lengths\n",
    "\n",
    "pair_df['amino_seq_1_len'] = pair_df['amino_seq_1'].str.len()\n",
    "pair_df['amino_seq_2_len'] = pair_df['amino_seq_2'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000\n"
     ]
    }
   ],
   "source": [
    "#Extract chosen subset dataframe\n",
    "\n",
    "pair_df_train = pair_df.iloc[train_index].copy().reset_index(drop=True)\n",
    "pair_df_test = pair_df.iloc[test_index].copy().reset_index(drop=True)\n",
    "\n",
    "pair_df_train['is_train'] = 1\n",
    "pair_df_train['is_test'] = 0\n",
    "\n",
    "pair_df_test['is_train'] = 0\n",
    "pair_df_test['is_test'] = 1\n",
    "\n",
    "data_df = pd.concat([pair_df_train, pair_df_test]).copy().reset_index(drop=True)\n",
    "\n",
    "data_df = data_df[[\n",
    "    'monomer_id_1',\n",
    "    'monomer_id_2',\n",
    "    'amino_seq_1',\n",
    "    'amino_seq_2',\n",
    "    'interacts',\n",
    "    'amino_seq_1_len',\n",
    "    'amino_seq_2_len',\n",
    "    'is_train',\n",
    "    'is_test'\n",
    "]]\n",
    "\n",
    "print(len(data_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"coiled_coil_binders_44000\"\n",
    "\n",
    "data_df.to_csv(dataset_name + \".csv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
